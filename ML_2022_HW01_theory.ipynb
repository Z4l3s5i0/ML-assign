{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4cc74",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda88da5",
   "metadata": {},
   "source": [
    "In this part of the homework, you are to solve several theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2c13b",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140cdcc1",
   "metadata": {},
   "source": [
    "## Task 1. Gradient Descent [6 points]\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be given by \n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}Q\\mathbf{x} - \\mathbf{x}^{\\top}\\mathbf{b}, $$\n",
    "\n",
    "where $\\mathbf{b} \\in \\mathbb{R}^n$ and $Q$ is a real symmetric positive-definite $n \\times n$ matrix.\n",
    "\n",
    "It is easy to see that the **minimum** of this function is reached at the point $\\mathbf{x}^{*} = Q^{-1}\\mathbf{b}$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Suppose that we apply the **gradient descent** method to this function with the **initial approximation** $\\mathbf{x}^{0} \\not= Q^{-1}\\mathbf{b}$.\n",
    "\n",
    "Show that method converges in one step that is $\\mathbf{x}^{1} = Q^{-1}\\mathbf{b}$, if and only if $\\mathbf{x}^{0}$ is chosen such that $\\mathbf{g}^{0} = Q\\mathbf{x}^{0} - \\mathbf{b}$ is an **eigenvector** of $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe70802",
   "metadata": {},
   "source": [
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2681b",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Let us consider $m$ data points $(\\mathbf{x}_i, y_i) \\in \\mathbb{R}^{d + 1}$ with feature vectors $\\mathbf{x}_i = \\begin{bmatrix} \\mathbf{x}_i^{1} \\\\ \\vdots \\\\ \\mathbf{x}_i^{d} \\end{bmatrix} \\in \\mathbb{R}^{d}$ and targets $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "When we want to use a linear approximation to predict target values, that is, in essence, find the weights $\\mathbf{w}^{\\top} = \\begin{bmatrix} w_0 \\\\ \\vdots \\\\ w_d \\end{bmatrix}$ in the linear combination of the features, such that the following objective is minimized\n",
    "\n",
    "$$\\mathcal{J}(\\mathbf{w}) = \\sum\\limits_{i = 1}^m \\Bigl(w_0 + w_1 \\cdot \\mathbf{x}_i^{1} + \\dots + w_d \\cdot \\mathbf{x}_i^{d} - y_i \\Bigr)^2 = \\sum\\limits_{i = 1}^m \\Bigl(\\begin{bmatrix} 1 & \\mathbf{x}_i^{\\top} \\end{bmatrix} \\mathbf{w} - y_i \\Bigr)^2 \\to \\min\\limits_{\\mathbf{w}}$$\n",
    "\n",
    "We can rewrite objective function as\n",
    "\n",
    "$$\\mathcal{J}(\\mathbf{w}) = \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2,$$\n",
    "\n",
    "where $\\mathbf{X} = \\begin{bmatrix} 1 & \\mathbf{x}_1^{\\top} \\\\ \\vdots & \\vdots \\\\ 1 & \\mathbf{x}_m^{\\top} \\end{bmatrix} \\in \\mathbb{R}^{m \\times (1 + d)}$ and $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^{m}$\n",
    "\n",
    "The solution to this problem $\\mathbf{w} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$ is only guaranteed if $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible!\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fca30",
   "metadata": {},
   "source": [
    "## Task 2. Linear Ridge Regression [6 points]\n",
    "\n",
    "Not all features can play a role in the formation of the target value. For example, the price of a house is highly dependent on the area but may not depend at all on the number of dogs the owner has. To exclude unnecessary features and avoid overfitting, it is necessary to introduce regularization.\n",
    "\n",
    "One common way of doing this is to upgrade the optimization problem to the following form\n",
    "\n",
    "$$\\mathcal{J}(\\mathbf{w}) = \\sum\\limits_{i = 1}^m \\Bigl(w_0 + w_1 \\cdot \\mathbf{x}_i^{1} + \\dots + w_d \\cdot \\mathbf{x}_i^{d} - y_i \\Bigr)^2 + \\lambda \\sum\\limits_{i = 0}^d w_i^2 \\to \\min\\limits_{\\mathbf{w}},$$\n",
    "\n",
    "where $\\lambda \\ge 0$ is a **regularization parameter**.\n",
    "\n",
    "This problem **always** has a closed-form solution! Find it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d12b10",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2a8c7",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier [6 points]\n",
    "\n",
    "Let us consider **multiclass classification problem** with classes $C_1, \\dots, C_K$.\n",
    "\n",
    "Assume that all $d$ features $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ are **binary**, i.e. $x_{i} \\in \\{0, 1\\}$ for $i = \\overline{1, d}$ **or** feature vector $\\mathbf{x} \\in \\{0, 1\\}^d$.\n",
    "\n",
    "Show that the decision rule of a **Bayesian Naive Classifier** can be represented as $\\arg\\max$ of linear functions of the input.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Hint**: use the **maximum a posteriori** (MAP) decision rule: $\\hat{y} = \\arg\\max\\limits_{y \\in \\overline{1, K}} p(y)p(\\mathbf{x}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e710",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
