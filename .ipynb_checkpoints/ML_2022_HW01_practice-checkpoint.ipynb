{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca35e1a8",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fdd20",
   "metadata": {},
   "source": [
    "To solve this task efficiently, here are some practical suggestions:\n",
    "\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember that tutorials, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, the whole life?).\n",
    "\n",
    "\n",
    "* Instead of rewriting existing code use **BUILT-IN METHODS** available in the libraries. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "\n",
    "\n",
    "* To complete this part of the homework, you have to write some **CODE** directly inside the specified places in the notebook **CELLS**.\n",
    "\n",
    "\n",
    "* In some problems you are asked to provide a short discussion of the results. In these cases you have to create a **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
    "\n",
    "\n",
    "* For every separate problem, you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if a reviewer executes your code, the output will be the same (with all the corresponding plots) as in your uploaded files. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudo randomness.\n",
    "\n",
    "\n",
    "* Your code must be readable to any competent reviewer. For this purpose, try to include **necessary** (and not more) comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY**.\n",
    "\n",
    "\n",
    "* Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameter and set it to `-1` to use all the cores.\n",
    "\n",
    "\n",
    "* **IMPORTANT:** In the end you need to hand in a **single zip file** containing **two notebooks** (theory and practice) as well as the **html exported versions** of these notebooks. That is **4** files in total.\n",
    "\n",
    "\n",
    "To begin let's import the essential (for this assignment) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a046d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default parameters for plots\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46582c2",
   "metadata": {},
   "source": [
    "# Numpy and Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93365ffd",
   "metadata": {},
   "source": [
    "## Task 1. Numpy Standardization [6 points]\n",
    "\n",
    "Write a function that takes a matrix (numpy array) as input and returns a matrix in which each **column** is standardized.\n",
    "\n",
    "Check [this](https://docs.scipy.org/doc/numpy/reference/routines.statistics.html) out for documentation. You are **only allowed** to use the **numpy** library! You are **not allowed** to use any loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc57544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(matrix):\n",
    "    assert matrix.ndim == 2, \"Check the input matrix shape!\"\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "\n",
    "    column_standardized_matrix = matrix / np.linalg.norm(matrix)\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return column_standardized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "936e8ddb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Houston, we have a problem!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This is a simple test to check the your function works as desired\u001b[39;00m\n\u001b[0;32m      3\u001b[0m matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(standardize(matrix),\n\u001b[0;32m      6\u001b[0m                    StandardScaler()\u001b[38;5;241m.\u001b[39mfit(matrix)\u001b[38;5;241m.\u001b[39mtransform(matrix),\n\u001b[0;32m      7\u001b[0m                    atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouston, we have a problem!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWell done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Houston, we have a problem!"
     ]
    }
   ],
   "source": [
    "# This is a simple test to check the your function works as desired\n",
    "\n",
    "matrix = np.random.randint(low=-10, high=10, size=(3, 2))\n",
    "\n",
    "assert np.allclose(standardize(matrix),\n",
    "                   StandardScaler().fit(matrix).transform(matrix),\n",
    "                   atol=1e-8), \"Houston, we have a problem!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0c0e9",
   "metadata": {},
   "source": [
    "## Task 2. Statistics [7 points]\n",
    "\n",
    "Repeat the following experiment **1000 times**:\n",
    "\n",
    "1. generate two **10 x 10** matrices from the **standard normal distribution**,\n",
    "\n",
    "2. multiply them (as matrices) and\n",
    "\n",
    "3. find the **maximum** element.\n",
    "\n",
    "Draw the **histogram** of max elements.\n",
    "\n",
    "You are **allowed** to use a **loop** only to repeat the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee03537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_sampling(n_rows, n_columns):\n",
    "    \"\"\"\n",
    "    Generates a random matrix of size [n_rows, n_columns].\n",
    "    \"\"\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(x, y):\n",
    "    \"\"\"\n",
    "    Multipies matrices x and y.\n",
    "    \"\"\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "\n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404492c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_element(matrix):\n",
    "    \"\"\"\n",
    "    Returns the maximum element of the matrix.\n",
    "    \"\"\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "\n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53094431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(n_rows, n_columns):\n",
    "    \"\"\"\n",
    "    Generates 2 random matrices of size [n_rows, n_columns].\n",
    "    And returns the maximum element of their product.\n",
    "    Use functions defined above.\n",
    "    \"\"\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return max_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b35014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the experiment N times and plot the histogram of the max element.\n",
    "\"\"\"\n",
    "\n",
    "N = 1000\n",
    "n_rows, n_columns = 10, 10\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "### END Solution\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502626d",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In the **binary classification** problem objects (feature vectors) $\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{m} \\in \\mathbb{R}^d$ have binary labels $y_{1}, \\dots, y_{m} \\in \\{0, 1\\}$.\n",
    "\n",
    "Using a **linear combination** of the features $\\mathbf{\\theta}^\\top \\mathbf{x}$, like in Least Mean Squares, will result in an unbounded estimator. However, we would like to have a mapping $f_{\\mathbf{\\theta}}: \\mathbb{R}^d \\to \\{0, 1\\}$, the output of which can be naturally interpreted as the probability of belonging to class 1.\n",
    "\n",
    "In **Logistic Regression** the resulting dot-product $\\mathbf{\\theta}^\\top \\mathbf{x}$ is converted to the unit interval with the **sigmoid** function:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This gives us the hypothesis function:\n",
    "\n",
    "$$f_{\\mathbf{\\theta}}(\\mathbf{x}) = g(\\mathbf{\\theta}^{\\top}\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{\\theta}^{\\top}\\mathbf{x}}}$$\n",
    "\n",
    "Now, we only need to set a **threshold** (for example, 0.5) for classifying an object to the 1st class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecebedd",
   "metadata": {},
   "source": [
    "## Task 3. Sigmoid [5 points]\n",
    "\n",
    "Implement and plot the sigmoid function. \n",
    "\n",
    "**Important!** Your function should work for inputs of arbitrary shape. The sigmoid should be applied elementwise. The returned array should have the same shape as the input. \n",
    "\n",
    "**Important!!** For large negative input, computing the exponent in the sigmoid may result in overflow. Use an alternative form of the sigmoid for the negative entries of the input to deal with this issue.\n",
    "\n",
    "**Hint:** An alternative form can be obtained by multiplying the nominator and the denominator of the sigmoid by $e^z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### BEGIN Solution\n",
    "\n",
    "\n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should not raise a warning\n",
    "sigmoid(np.array([-1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "### END Solution\n",
    "\n",
    "plt.title('Sigmoid', size=16)\n",
    "\n",
    "plt.xlabel('z', size=14)\n",
    "plt.ylabel('sigmoid(z)', size=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609491e",
   "metadata": {},
   "source": [
    "Thus, the conditional probabilities of belonging to class 1 or 0 are as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "    p(y = 1| \\mathbf{x}; \\mathbf{\\theta}) &= f_{\\mathbf{\\theta}}(\\mathbf{x}) \\\\\n",
    "    p(y = 0| \\mathbf{x}; \\mathbf{\\theta}) &= 1 - f_{\\mathbf{\\theta}}(\\mathbf{x})\n",
    "\\end{align}$$\n",
    "\n",
    "Or one could rewrite it as:\n",
    "\n",
    "$$p(y| \\mathbf{x}; \\mathbf{\\theta}) = f_{\\mathbf{\\theta}}(\\mathbf{x})^{y}\\bigl[1 - f_{\\mathbf{\\theta}}(\\mathbf{x})\\bigr]^{1 - y}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Now, to **estimate** the weights $\\mathbf{\\theta}$, we will **maximize** the **likelihood** function (more precisely, its logarithm for simpler calculations). Therefore, this method is called the **maximum likelihood estimation** (MLE).\n",
    "\n",
    "$$\\ln{\\mathcal{L}(\\mathbf{\\theta})} = \\mathcal{l}(\\mathbf{\\theta}) = \\sum\\limits_{i = 1}^m p(y_i| \\mathbf{x}_i; \\mathbf{\\theta}) = \\sum\\limits_{i = 1}^m y_i \\ln{\\bigl[f_{\\mathbf{\\theta}}(\\mathbf{x}_i) \\bigr]} + (1 - y_i)\\ln{\\bigl[1 - f_{\\mathbf{\\theta}}(\\mathbf{x}_i) \\bigr]} \\to \\max\\limits_{\\mathbf{\\theta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9369a",
   "metadata": {},
   "source": [
    "## Task 4. Log-Likelihood [4 points]\n",
    "\n",
    "Let $\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^{\\top} \\\\ \\vdots \\\\ \\mathbf{x}_m^{\\top} \\end{bmatrix} \\in \\mathbb{R}^{m \\times d}$ be the data matrix, $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^{m}$ be the labels vector corresponding to $\\mathbf{X}$ and $\\mathbf{\\theta} = \\begin{bmatrix} \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{bmatrix} \\in \\mathbb{R}^{d}$ be the parameters vector.\n",
    "\n",
    "Implement the log-likelihood for Logistic Regression.\n",
    "\n",
    "**Note:** You have already implemented the sigmoid function. Now it is time to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_log_reg(theta, X, y):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "\n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98139a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "d = 2\n",
    "\n",
    "X = np.random.randint(low=-10, high=10, size=(m, d))\n",
    "y = np.random.randint(low=0, high=1, size=(m, 1))\n",
    "theta = np.zeros((d, 1))\n",
    "\n",
    "assert np.allclose(log_likelihood_log_reg(theta, X, y),\n",
    "                   -m * np.log(2),\n",
    "                   atol=1e-8), \"Houston, we have a problem!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5bf54",
   "metadata": {},
   "source": [
    "## Task 5. Gradient Descent [10 points]\n",
    "\n",
    "Now, to find the weights $\\mathbf{\\theta}$, we need to solve the optimization problem described above. Let's use [**gradient descent**](https://en.wikipedia.org/wiki/Gradient_descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203a7e2",
   "metadata": {},
   "source": [
    "Since the gradient descent solves the **minimization** problem, we will change the maximization problem described above to the minimization problem by changing the sign of the log-likelihood function to negative:\n",
    "\n",
    "$$\\mathcal{l}(\\mathbf{\\theta}) \\to \\max\\limits_{\\mathbf{\\theta}} \\Longleftrightarrow -\\mathcal{l}(\\mathbf{\\theta}) \\to \\min\\limits_{\\mathbf{\\theta}}$$\n",
    "\n",
    "So let's first implement a function to compute the gradient of the **negative** log-likelihood function.\n",
    "\n",
    "The gradient is a column vector that has **the same shape as** $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_gradient_log_reg(theta, X, y):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "\n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f48b8e",
   "metadata": {},
   "source": [
    "Now implement the general gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4149b76",
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient_descent(X, y, gradient, theta_0, alpha=0.01, tolerance=1e-8, max_iters_number=100):\n",
    "    \"\"\"\n",
    "    X: data matrix of shape [m, d]\n",
    "    y: labels of shape [m]\n",
    "    gradient: a function to compute the gradient of the neg. log-likelihood\n",
    "    theta_0: initialization of theta of shape [d]\n",
    "    alpha: learning rate\n",
    "    tolerance: a value to detect convergence (if the norm of the update is smaller that tolerance, terminate)\n",
    "    max_iters_number: maximum number of iterations of the algorithm\n",
    "    \n",
    "    return: the final estimation for theta\n",
    "    \"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = y[:, np.newaxis]\n",
    "    \n",
    "    if theta_0.ndim == 1:\n",
    "        theta_0 = theta_0[:, np.newaxis]\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    for ...:\n",
    "        ...\n",
    "        \n",
    "        # termination\n",
    "        if ...:  # some criteria for stopping\n",
    "            ...\n",
    "        \n",
    "    ...\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b2c17",
   "metadata": {},
   "source": [
    "## Task 6. Classifier [10 points]\n",
    "\n",
    "Let's create our own classifier class and then compare it with the [existing method in the sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "We need to implement three principal methods:\n",
    "\n",
    "* `fit` to find the coefficients (weights) $\\mathbf{\\theta}$\n",
    "\n",
    "\n",
    "* `predict` to predict the labels $\\mathbf{\\hat{y}}$ for the data matrix $\\mathbf{X}$\n",
    "\n",
    "\n",
    "* `score` to evaluate predictions (for example, with **mean accuracy** score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(predictions, labels):\n",
    "    \"\"\"\n",
    "    Returns the accuracy of predictions when compared to the true labels\n",
    "    \"\"\"\n",
    "    assert predictions.shape == labels.shape, \"Check shapes!\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ### BEGIN Solution\n",
    "        theta_0 = ...  # initialize with random noise\n",
    "        \n",
    "        self.coef_ = ...  # calculate using gradient descent\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        predictions = ...\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "        assert predictions.shape == (X.shape[0],), \"Check shapes!\"\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551ddea",
   "metadata": {},
   "source": [
    "## Task 7. Decision Rule [7 points]\n",
    "\n",
    "In this task, your goal is to visualize the **decision rule** of **Logistic Regression** applied to a synthetic $2$-dimensional dataset generated by a built-in `sklearn.datasets` method called `make_moons`. In the cell below we generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a96692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2d2bd",
   "metadata": {},
   "source": [
    "The goal is to fit our LogReg and sklearn's Logistic Regreesion classifiers to this data:\n",
    "\n",
    "You have to plot the decision regions. The plots must have **titles**, which contain the names of the classifiers and the corresponding accuracy (rounded to only **two** decimal places).\n",
    "\n",
    "You can write the plotting code on your own, but we highly recommend just to use [mlxtend](http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/) library (`pip install mlxtend`), which has a awesome one-line decision boundary plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ceb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64657e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_logistic_regression = LogReg()\n",
    "logistic_regression = LogisticRegression(random_state=0, solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "classifiers = [my_logistic_regression, logistic_regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(16, 8))\n",
    "\n",
    "for classifier, axis in zip(classifiers, axes.flat):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    # Fit the classifier, and plot the decision regions\n",
    "\n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25318d8d",
   "metadata": {},
   "source": [
    "## Task 8. Feature Engineering [7 points]\n",
    "\n",
    "In the previous task, classifiers obviously failed fitting to data. This happened because the decision boundary of the classifier has a restricted linear form, while the data is much more complicated.\n",
    "\n",
    "One may try to change the parameters of the classifier in order to improve accuracy, but linear models do not have parameters that can change the form of the decision rule.\n",
    "\n",
    "In this case, the **feature engineering** helps: one may try to compute new (e.g. non-linear) features based on the existing ones and fit the classifier for the new features. This may help low-complexity classifiers to fit complex data dependencies.\n",
    "\n",
    "Your task is\n",
    "\n",
    "* to achieve accuracy $> 0.95$, by generating additional features (e.g. polynomial),\n",
    "\n",
    "\n",
    "* to plot decision rules in the original feature space,\n",
    "\n",
    "\n",
    "* to write 2-3 sentences about why you chose these features.\n",
    "\n",
    "It is your choice how to generate features. You may create hand-crafted features and add them manually.\n",
    "\n",
    "Nevertheless, we **highly recommend** getting used to and applying the following built-in `sklearn` methods, for example:\n",
    "\n",
    "* `PolynomialFeatures` for [feature generation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "* `StandardScaler`for [feature scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "* `Pipeline` - for [combining several operations](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in a row (e.g. feature creation & prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c78e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_simple = Pipeline([('LR', LogisticRegression(random_state=0, solver='lbfgs', n_jobs=-1))])\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "logistic_regression_advanced = ...\n",
    "\n",
    "### END Solution\n",
    "\n",
    "classifiers = [logistic_regression_simple, logistic_regression_advanced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(16, 8))\n",
    "\n",
    "for classifier, axis in zip(classifiers, axes.flat):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    # Fit the classifier, and plot the decision regions\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff096dd",
   "metadata": {},
   "source": [
    "Why did you choose these features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2944c",
   "metadata": {},
   "source": [
    "# Face classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293a18b",
   "metadata": {},
   "source": [
    "In this task you will face a real-life problem of face detection. You have to train a model to classify 24$\\times$24 grayscale images to *face*/*non-face* classes.\n",
    "First, let us import some libraries and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set default parameters for plots\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = loadmat('faces.mat')\n",
    "labels = np.squeeze(data['Labels'])\n",
    "labels[labels == -1] = 0  # Want labels in {0, 1}\n",
    "data = data['Data']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b5216",
   "metadata": {},
   "source": [
    "Each datapoint is a 576-dimentional vector that stores pixel intensities of a flattened grayscale image.\n",
    "If carefully reshaped, one can visualize the datapoints as 24$\\times$24 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89247802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "samples_per_class = 10\n",
    "classes = [0, 1]\n",
    "imgs = np.reshape(data, [-1, 24, 24], order='F')\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.equal(labels, cls))\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = y * samples_per_class + i + 1\n",
    "        plt.subplot(len(classes), samples_per_class, plt_idx)\n",
    "        plt.imshow(imgs[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e5d8e",
   "metadata": {},
   "source": [
    "Now let us split the dataset into train and test. This will allow to assess the ability of our models to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40963a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb168d9b",
   "metadata": {},
   "source": [
    "## Task 9. Logistic Regression for Face Classification [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee902403",
   "metadata": {},
   "source": [
    "Now fit your *LogReg* class on *(X_train, y_train)* and report the accuracy on both the **train** and **test** sets.\n",
    "\n",
    "**Warning:** It may take time to fit your model to this amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d18382",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03064d",
   "metadata": {},
   "source": [
    "Visualize the learned coefficients in a shape of a 24$\\times$24 grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f167905",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af0884",
   "metadata": {},
   "source": [
    "## Task 10. Linear Discriminant Analysis for Face Classification [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984d6e2",
   "metadata": {},
   "source": [
    "From the lectures you've learned about another linear model for classification, Linear (Gaussian) Discriminant Analysis.\n",
    "In this task you are asked to implement that model and fit it to the face dataset. Report the accuracy of your implementation on the training/test sets and compare it to the build-in sklearn model (*sklearn.discriminant_analysis.LinearDiscriminantAnalysis*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc096fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        self.bias_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: Data matrix of shape [m, num_features]\n",
    "        y: Labels corresponding to X of shape [m,]\n",
    "        \"\"\"\n",
    "        \n",
    "        phi = None  # The prior probability of the first class\n",
    "        mu_0 = None  # The mean of the first class\n",
    "        mu_1 = None  # The mean of the second class\n",
    "        sigma = None  # The shared covariance matrix\n",
    "        \n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        # Estimate the values of phi, mu_0, mu_1 and sigma from the data\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "        # Compute the coefs from the results of GDA\n",
    "        sigma_inv = np.linalg.inv(sigma)\n",
    "        quad_form = lambda A, x: np.dot(x.T, np.dot(A, x))\n",
    "        self.bias_ = 0.5 * quad_form(sigma_inv, mu_0) - 0.5 * quad_form(sigma_inv, mu_1) + np.log(phi / (1 - phi))\n",
    "        self.coef_ = np.dot(sigma_inv, (mu_1 - mu_0))\n",
    "\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        predictions = ...\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        # Compute the accuracy of the classifier on X, y\n",
    "        \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490201e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", lda.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", lda.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05021791",
   "metadata": {},
   "source": [
    "Visualize the learned coefficients in a shape of a 24$\\times$24 grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc261a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe18b3",
   "metadata": {},
   "source": [
    "## Task 11. Tuning the model [5 points]\n",
    "\n",
    "The final task is to build a model that reaches as high accuracy on the test set as possible.\n",
    "Feel free to use anything that you already know from the lectures/tutorials/this assignment by the time this assignment was given (i.e. the 4th week).\n",
    "\n",
    "Justify your final choice of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
